{
  "aiPlatforms": {
    "claude": {
      "endpoint": "https://api.anthropic.com/v1/messages",
      "minTemperature": -1.0,
      "maxTemperature": 1.0,
      "temperature": 0.5,
      "topP": 0.7,
      "models": [
        {
          "id": "claude-3-7-sonnet-latest",
          "description": "Most intelligent model, first hybrid reasoning model. Features deep thinking.",
          "maxTokens": 8192,
          "parameterStyle": "standard",
          "contextWindow": 200000,
          "inputTokenPrice": 3.00,
          "outputTokenPrice": 15.00,
          "supportsTemperature": true,
          "supportsTopP": true
        },
        {
          "id": "claude-3-5-haiku-latest",
          "description": "Fastest model, balancing speed and intelligence.",
          "maxTokens": 8192,
          "parameterStyle": "standard",
          "contextWindow": 200000,
          "inputTokenPrice": 0.80,
          "outputTokenPrice": 4.00,
          "supportsTemperature": true,
          "supportsTopP": true
        },
        {
          "id": "claude-3-5-sonnet-latest",
          "description": "High-intelligence model balancing capability and speed.",
          "maxTokens": 8192,
          "parameterStyle": "standard",
          "contextWindow": 200000,
          "inputTokenPrice": 3.00,
          "outputTokenPrice": 15.00,
          "supportsTemperature": true,
          "supportsTopP": true
        },
        {
          "id": "claude-3-opus-latest",
          "description": "Powerful model for complex tasks. Top-level intelligence and understanding.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 200000,
          "inputTokenPrice": 15.00,
          "outputTokenPrice": 75.00,
          "supportsTemperature": true,
          "supportsTopP": true
        }
      ],
      "defaultModel": "claude-3-5-haiku-latest",
      "requiresModel": true,
      "authType": "header",
      "authHeaderName": "x-api-key",
      "hasSystemPrompt": true
    },
    "chatgpt": {
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "minTemperature": 0.0,
      "maxTemperature": 2.0,
      "temperature": 0.5,
      "topP": 0.9,
      "models": [
        {
          "id": "gpt-4.5-preview",
          "description": "Largest and most capable flagship GPT model (Preview).",
          "maxTokens": 16384,
          "parameterStyle": "standard",
          "contextWindow": 128000,
          "inputTokenPrice": 75.00,
          "outputTokenPrice": 150.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "gpt-4o",
          "description": "Fast, intelligent, and flexible flagship model.",
          "maxTokens": 16384,
          "parameterStyle": "standard",
          "contextWindow": 128000,
          "inputTokenPrice": 2.50,
          "outputTokenPrice": 10.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "gpt-4o-mini",
          "description": "Fast and affordable small model, optimized for focused tasks.",
          "maxTokens": 16384,
          "parameterStyle": "standard",
          "contextWindow": 128000,
          "inputTokenPrice": 0.15,
          "outputTokenPrice": 0.60,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "o1",
          "description": "High-intelligence reasoning model for complex, multi-step tasks.",
          "maxTokens": 100000,
          "parameterStyle": "reasoning",
          "tokenParameter": "max_completion_tokens",
          "contextWindow": 200000,
          "inputTokenPrice": 15.00,
          "outputTokenPrice": 60.00,
          "supportsTemperature": false,
          "supportsTopP": false,
          "supportsSystemPrompt": false
        },
        {
          "id": "o3-mini",
          "description": "Fast, flexible, and intelligent reasoning model (o-series).",
          "maxTokens": 100000,
          "parameterStyle": "reasoning",
          "tokenParameter": "max_completion_tokens",
          "contextWindow": 200000,
          "inputTokenPrice": 1.10,
          "outputTokenPrice": 4.40,
          "supportsTemperature": false,
          "supportsTopP": false
        },
        {
          "id": "o1-mini",
          "description": "Faster, more affordable reasoning model compared to o1.",
          "maxTokens": 65536,
          "parameterStyle": "reasoning",
          "tokenParameter": "max_completion_tokens",
          "contextWindow": 128000,
          "inputTokenPrice": 1.10,
          "outputTokenPrice": 4.40,
          "supportsTemperature": false,
          "supportsTopP": false,
          "supportsSystemPrompt": false
        },
        {
          "id": "gpt-4-turbo",
          "description": "Older high-intelligence GPT model.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 128000,
          "inputTokenPrice": 10.00,
          "outputTokenPrice": 30.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "gpt-3.5-turbo",
          "description": "Legacy model, cost-effective for various chat and non-chat tasks.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 16385,
          "inputTokenPrice": 0.50,
          "outputTokenPrice": 1.50,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        }
      ],
      "defaultModel": "gpt-4o-mini",
      "requiresModel": true,
      "authType": "bearer",
      "authHeaderName": "Authorization",
      "hasSystemPrompt": true
    },
    "deepseek": {
      "endpoint": "https://api.deepseek.com/v1/chat/completions",
      "minTemperature": 0.0,
      "maxTemperature": 2.0,
      "minTopP": 0.1,
      "maxTopP": 1.0,
      "temperature": 1.0,
      "topP": 0.9,
      "models": [
        {
          "id": "deepseek-chat",
          "description": "General chat model (DeepSeek-V3).",
          "maxTokens": 8000,
          "parameterStyle": "standard",
          "contextWindow": 64000,
          "inputTokenPrice": 0.07,
          "outputTokenPrice": 1.10,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "deepseek-reasoner",
          "description": "Reasoning model (DeepSeek-R1) featuring Chain of Thought capabilities.",
          "maxTokens": 8000,
          "parameterStyle": "standard",
          "contextWindow": 64000,
          "inputTokenPrice": 0.14,
          "outputTokenPrice": 2.19,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        }
      ],
      "defaultModel": "deepseek-chat",
      "requiresModel": true,
      "authType": "bearer",
      "authHeaderName": "Authorization",
      "hasSystemPrompt": true
    },
    "mistral": {
      "endpoint": "https://api.mistral.ai/v1/chat/completions",
      "minTemperature": 0.0,
      "maxTemperature": 1.5,
      "temperature": 0.3,
      "topP": 0.7,
      "models": [
        {
          "id": "mistral-large-latest",
          "description": "Top-tier reasoning model for complex tasks.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 131000,
          "inputTokenPrice": 2.00,
          "outputTokenPrice": 6.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "mistral-small-latest",
          "description": "Leading small model.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 131000,
          "inputTokenPrice": 0.00,
          "outputTokenPrice": 0.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "codestral-latest",
          "description": "Cutting-edge model specialized for coding tasks.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 256000,
          "inputTokenPrice": 0.30,
          "outputTokenPrice": 0.90,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "mistral-saba-latest",
          "description": "Efficient model focused on Middle East & South Asia languages.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 32000,
          "inputTokenPrice": 0.20,
          "outputTokenPrice": 0.60,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        }
      ],
      "defaultModel": "mistral-small-latest",
      "requiresModel": true,
      "authType": "bearer",
      "authHeaderName": "Authorization",
      "hasSystemPrompt": true
    },
    "gemini": {
      "endpoint": "https://generativelanguage.googleapis.com/v1/models/{model}:generateContent",
      "minTemperature": 0.0,
      "maxTemperature": 2.0,
      "temperature": 0.7,
      "topP": 0.9,
      "models": [
        {
          "id": "gemini-2.5-pro-exp-03-25",
          "description": "Most powerful reasoning model (Experimental Mar 2025). Peak performance.",
          "maxTokens": 65536,
          "parameterStyle": "standard",
          "contextWindow": 1048576,
          "inputTokenPrice": 0.00,
          "outputTokenPrice": 0.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "maxOutputTokens",
          "supportsSystemPrompt": true
        },
        {
          "id": "gemini-2.0-flash",
          "description": "Low latency, enhanced performance.",
          "maxTokens": 8192,
          "parameterStyle": "standard",
          "contextWindow": 1048576,
          "inputTokenPrice": 0.00,
          "outputTokenPrice": 0.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "supportsSystemPrompt": false,
          "tokenParameter": "maxOutputTokens"
        },
        {
          "id": "gemini-2.0-flash-lite",
          "description": "Cost-effective and low-latency version of Gemini 2.0 Flash.",
          "maxTokens": 8192,
          "parameterStyle": "standard",
          "contextWindow": 1048576,
          "inputTokenPrice": 0.00,
          "outputTokenPrice": 0.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "supportsSystemPrompt": false,
          "tokenParameter": "maxOutputTokens"
        },
        {
          "id": "gemini-1.5-flash",
          "description": "Fast and versatile model for diverse tasks.",
          "maxTokens": 8192,
          "parameterStyle": "standard",
          "contextWindow": 1048576,
          "inputTokenPrice": 0.00,
          "outputTokenPrice": 0.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "supportsSystemPrompt": false,
          "tokenParameter": "maxOutputTokens"
        },
        {
          "id": "gemini-1.5-flash-8b",
          "description": "Optimized for high-volume, lower-intelligence tasks.",
          "maxTokens": 8192,
          "parameterStyle": "standard",
          "contextWindow": 1048576,
          "inputTokenPrice": 0.00,
          "outputTokenPrice": 0.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "supportsSystemPrompt": false,
          "tokenParameter": "maxOutputTokens"
        },
        {
          "id": "gemini-1.5-pro",
          "description": "Powerful model for complex reasoning tasks requiring high intelligence.",
          "maxTokens": 8192,
          "parameterStyle": "standard",
          "contextWindow": 2097152,
          "inputTokenPrice": 0.00,
          "outputTokenPrice": 0.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "supportsSystemPrompt": false,
          "tokenParameter": "maxOutputTokens"
        }
      ],
      "defaultModel": "gemini-2.0-flash",
      "requiresModel": true,
      "authType": "query",
      "authParamName": "key",
      "hasSystemPrompt": true
    },
    "grok": {
      "endpoint": "https://api.x.ai/v1/chat/completions",
      "minTemperature": 0.0,
      "maxTemperature": 2.0,
      "minTopP": 0.01,
      "maxTopP": 1.0,
      "temperature": 0.7,
      "topP": 0.9,
      "models": [
         {
          "id": "grok-3-latest",
          "description": "Flagship model (Beta) excelling at enterprise tasks (data extraction, coding, summarization) with deep domain knowledge.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 131072,
          "inputTokenPrice": 3.00,
          "outputTokenPrice": 15.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "grok-3-fast-latest",
          "description": "Faster version of Grok 3, offering significantly lower latency for the same quality at a higher cost.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 131072,
          "inputTokenPrice": 5.00,
          "outputTokenPrice": 25.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "grok-3-mini-latest",
          "description": "Lightweight Grok 3 model with reasoning ('thinking'). Fast, smart, good for logic tasks without deep domain needs.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 131072,
          "inputTokenPrice": 0.30,
          "outputTokenPrice": 0.50,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        },
        {
          "id": "grok-3-mini-fast-latest",
          "description": "Faster version of Grok 3 Mini, offering significantly lower latency for the same quality at a higher cost.",
          "maxTokens": 4096,
          "parameterStyle": "standard",
          "contextWindow": 131072,
          "inputTokenPrice": 0.60,
          "outputTokenPrice": 4.00,
          "supportsTemperature": true,
          "supportsTopP": true,
          "tokenParameter": "max_tokens"
        }
      ],
      "defaultModel": "grok-3-mini-latest",
      "requiresModel": true,
      "authType": "bearer",
      "authHeaderName": "Authorization",
      "hasSystemPrompt": true
    }
  }
}